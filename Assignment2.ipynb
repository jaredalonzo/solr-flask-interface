{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We will be working with Amazon review data again, but this time the data will come from files, not from scraping web pages.\n",
    "Relevant data is split across two files / schemas\n",
    "\n",
    "* Product data -- information about a product, keyed by ASIN.  Has information like the product's name, price, browse categories, and sales rank \n",
    "* Review data -- information about a single review.  Has information like the reviewer's name and ID, the product ASIN, the review's summary, body, score, and \"helpfulness\"\n",
    "\n",
    "The two files are linked by ASIN, and every ASIN that is the ASIN of a review is guaranteed to appear in the product data file.  It is not guaranteed that every entry in the ASIN file has any reviews.\n",
    "\n",
    "This notebook will guide you through the process of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "### Fields and Their Types\n",
    "\n",
    "#### Reviews\n",
    "\n",
    "| Field | Type | Note |\n",
    "|-------|------|------|\n",
    "| id | UUID | Not in the input file;  supplied by SOLR\n",
    "| reviewerID | ignore |\n",
    "| asin | string | Joins with the asin field in the product file |\n",
    "| reviewerName | string | ignore |\n",
    "| helpful | ignore |\n",
    "| reviewText | text | Full review body |\n",
    "| overall | integer | Truncate the floating-point value |\n",
    "| summary | text | Review summary text |\n",
    "| unixReviewTime | ignore |\n",
    "| reviewTime | ignore |\n",
    "\n",
    "#### Products\n",
    " \n",
    "| Field | Type | Note |\n",
    "|-------|------|------|\n",
    "| asin | string | Unique ID for products.  Joins with the asin field in the reviews file. |\n",
    "| description | string | Stored but not indexed.  Shown on product detail page. |\n",
    "| title | string | Stored but not indexed.  Shown on the product detail page and also on review search result and detail pages |\n",
    "| imUrl | ignore |  |\n",
    "| price | float | Displayed in currency format on the product detail page.|\n",
    "| salesRank | ignore |  |\n",
    "| categories | ignore | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Elements\n",
    "\n",
    "### Loading the Data Files\n",
    "\n",
    "Content in the data files is one line per \"data row.\"  Each line can be converted to a Python dictionary using this \n",
    "code:\n",
    "\n",
    "``eval('(' + line + ')')``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Files and Indexing\n",
    "\n",
    "This code will look very similar to Assignment 1.  This code should parse and index all review and product records.  Notice that the code takes two file names, one for products and one for reviews.  I may run your code on different data sets, but they will be in the same format as the files provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "SOLR_EXECUTABLE = '/usr/local/Cellar/solr/8.0.0/bin/solr'\n",
    "def solr_command(*args):\n",
    "    return subprocess.check_output([SOLR_EXECUTABLE] + list(args))\n",
    "def create_collection(config_dir, collection_name):\n",
    "    solr_command('create_core', '-c', collection_name, '-d', config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_collection('/Users/jaredalonzo/Computer Science/CS4910 Text Processing & Search/hw2-submission/amazon-reviews', 'amazon-reviews')\n",
    "create_collection('/Users/jaredalonzo/Computer Science/CS4910 Text Processing & Search/hw2-submission/amazon-products', 'amazon-products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  These two functions create a list of documents to be indexed -- each a dictionary \n",
    "#  that can be passed to your indexing functions below.  You should do the following checks:\n",
    "#   -- skip any product that does not have a title\n",
    "#   -- skip any review that does not have a product \n",
    "#  Notice that these have to be done in sequence, as you won't know your list of products until \n",
    "#  the first step is finished\n",
    "def product_json(filename):\n",
    "    list_of_dictionaries = []\n",
    "    for item in open(filename):\n",
    "        dict = {}\n",
    "        line = eval('(' + item + ')')\n",
    "        if 'title' in line:\n",
    "            dict.update({'title': line['title']})\n",
    "        else:\n",
    "            continue\n",
    "        if 'description' in line:\n",
    "            dict.update({'description': line['description']})\n",
    "        if 'price' in line:\n",
    "            dict.update({'price': line['price']})\n",
    "        dict.update({'asin': line['asin']})\n",
    "        list_of_dictionaries.append(dict)\n",
    "    return list_of_dictionaries\n",
    "\n",
    "def review_json(filename, products):\n",
    "    list_of_dictionaries = []\n",
    "    for item in open(filename):\n",
    "        dict = {}\n",
    "        line = eval('(' + item + ')')\n",
    "        if line['asin'] in products:\n",
    "            dict.update({'reviewText': line['reviewText']})\n",
    "            dict.update({'overall': int(line['overall'])})\n",
    "            dict.update({'summary': line['summary']})\n",
    "            dict.update({'asin': line['asin']})\n",
    "            list_of_dictionaries.append(dict)\n",
    "    return list_of_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "products = product_json(\"medium_asin_data.txt\")\n",
    "reviews = review_json(\"medium_review_data.txt\", [x['asin'] for x in products])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pysolr\n",
    "def index(data, port=8983, collection_name=''):\n",
    "    solr = pysolr.Solr(f'http://localhost:{port}/solr/{collection_name}')\n",
    "    solr.add(data, commit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index(reviews, collection_name='amazon-reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index(products, collection_name='amazon-products')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
